{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wRUesTuusfFX"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import pickle\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "\n",
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from tesorflow.keras.layers import LSTM, Dense, Activation\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "\n",
        "text_df = pd.read_csv(\"fake_or_real_news.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = list(text_df.text.values)\n",
        "joined_text = \" \".join(text)\n",
        "partial_text = joined_text[:10000]\n",
        "\n",
        "tokenizer = RegexpTokenizer(\"r\\w+\")\n",
        "tokens = tokenizer.tokenize(partial_text.lower())\n"
      ],
      "metadata": {
        "id": "4WPBUmWXtRcu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unique_tokens = np.unique(tokens)\n",
        "unique_token_index = {token: idx for idx, token in enumerate(unique_tokens)}\n"
      ],
      "metadata": {
        "id": "maDYz_NBt3JB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_words = 10\n",
        "input_words = []\n",
        "next_words = []\n",
        "\n",
        "\n",
        "for j in range(len(tokens) - n_words):\n",
        "  input_words.append(tokens[i:i+n_words])\n",
        "  next_words.append(tokens[i+n_words])\n",
        "\n",
        "\n",
        "X = np.zeros((len(input_words), n_words, len(unique_tokens)), d_type=bool)\n",
        "y = np.zeros((len(input_words), len(unique_tokens)), d_type=bool)\n",
        "\n"
      ],
      "metadata": {
        "id": "t5NcsWm9uHIb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, words in enumerate(input_words):\n",
        "  for j, word in enumerate(words):\n",
        "    X[i,j, unique_token_index[word]] = 1\n",
        "  y[i, unique_token_index[next_words[i]]] = 1\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Bab2Mq7ovkFM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model= Sequential()\n",
        "model.add(LSTM(128, input_shape=(n_words, len(unique_tokens)), return_sequences=True))\n",
        "model.add(LSTM (128))\n",
        "model.add(Dense(len (unique_tokens)))\n",
        "model.add(Activation (\"softmax\"))"
      ],
      "metadata": {
        "id": "CbwKkyGhw2xp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss=\"categorical_crossentropy\", optimizer RMSprop (learning_rate=0.01), metrics=[\"accuracy\"])\n",
        "model.fit(x, y, batch_size=128, epochs=10, shuffle=True)"
      ],
      "metadata": {
        "id": "01O21k-dxK8_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(\"mymodel.h5\")\n",
        "model = load_model(\"mymodel.h5\")\n",
        "\n",
        "def predict_next_word (input_text, n_best):\n",
        "  input_text input_text.lower()\n",
        "  X = np.zeros((1, n_words, len(unique_tokens)))\n",
        "  for i, word in enumerate(input_text.split()):\n",
        "    x[0, i, unique_token_index[word]] = 1\n",
        "  predictions model.predict(X) [0]\n",
        "\n",
        "\n",
        "  return np.argpartition (predictions, -n_best)[-n_best:]\n",
        "\n",
        "possible = predict_next_word(\"He will have to look into this thing and he\", 5)\n",
        "print([unique_tokens[idx] for idx in possible])"
      ],
      "metadata": {
        "id": "pkPazhSbxT_V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(input_text, text_length, creativity=3):\n",
        "  word_sequence input_text.split()\n",
        "  current = 0\n",
        "  for _ in range(text_length):\n",
        "    sub_sequence = \" \".join(tokenizer.tokenize(\" \".join(word_sequence).lower()) [current: current+n_words])\n",
        "  try:\n",
        "    choice = unique_tokens [random.choice (predict_next_word(sub_sequence,creativity))]\n",
        "\n",
        "  except:\n",
        "    choice = random.choice (unique_tokens)\n",
        "  word_sequence.append(choice)\n",
        "  current += 1\n",
        "\n",
        "  return \" \".join(word_sequence)\n",
        "#"
      ],
      "metadata": {
        "id": "JSWd4boCx4ml"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "kUGQYUxuyQ48"
      }
    }
  ]
}